{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "# from pyspark.ml.classification import Mul\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16062019t140304z\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    " \n",
    "TIMESTAMP = strftime(\"%d%m%Yt%H%M%Sz\", gmtime())\n",
    "print(TIMESTAMP)\n",
    "ID = \"vkbomb\" #my gfk ID    \n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[*]\") \\\n",
    ".appName(\"{}_app_{}\".format(ID,TIMESTAMP)) \\\n",
    ".config(\"spark.driver.memory\", \"500g\") \\\n",
    ".config(\"spark.executer.memory\", \"500g\") \\\n",
    ".config(\"spark.submit.deployMode\",\"client\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare training documents from a list of (id, text, label) tuples.\n",
    "# df_training = spark.createDataFrame([\n",
    "#     (0, \"a\", 1.0),\n",
    "#     (1, \"b\", 0.0),\n",
    "#     (2, \"a\", 1.0),\n",
    "#     (3, \"b\", 0.0)\n",
    "# ], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# # df_training = spark.createDataFrame([\n",
    "# #     (0, \"a b c d e spark\", 1.0),\n",
    "# #     (1, \"b d\", 0.0),\n",
    "# #     (2, \"spark f g h\", 1.0),\n",
    "# #     (3, \"hadoop mapreduce\", 0.0)\n",
    "# # ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "rf = RandomForestClassifier(labelCol='label',featuresCol='features', numTrees=1)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pipeline.fit(df_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "# df_test = spark.createDataFrame([\n",
    "#     (4, \"a\"),\n",
    "#     (5, \"b\"),\n",
    "#     (6, \"a\"),\n",
    "#     (7, \"c\")\n",
    "# ], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------------------+-------------+-----------+----------+\n",
      "| id|text|words|            features|rawPrediction|probability|prediction|\n",
      "+---+----+-----+--------------------+-------------+-----------+----------+\n",
      "|  4|   a|  [a]|(262144,[227410],...|    [0.0,1.0]|  [0.0,1.0]|       1.0|\n",
      "|  5|   b|  [b]|(262144,[30913],[...|    [1.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|  6|   a|  [a]|(262144,[227410],...|    [0.0,1.0]|  [0.0,1.0]|       1.0|\n",
      "|  7|   c|  [c]|(262144,[28698],[...|    [0.0,1.0]|  [0.0,1.0]|       1.0|\n",
      "+---+----+-----+--------------------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Make predictions on test documents and print columns of interest.\n",
    "# prediction = model.transform(df_test)\n",
    "# prediction.show()\n",
    "# # selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "# # for row in selected.collect():\n",
    "# #     rid, text, prob, prediction = row\n",
    "# #     print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sdf = spark.read.csv(\"file:////home/vkbomb/working/data/train_32353_new_cleaned_item_id.csv\",sep=\",\",header=True)\n",
    "\n",
    "sdf_filtered = sdf.select(\"main_text\",\"item_id\")\n",
    "sdf_filtered = sdf_filtered.withColumn(\"item_id\",sdf_filtered[\"item_id\"].cast(IntegerType())).fillna(0)\n",
    "# sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_test = spark.read.csv(\"file:////home/vkbomb/working/data/test_32353_new.csv\",sep=\",\",header=True)\n",
    "# sdf_test.printSchema()\n",
    "sdf_test = sdf_test.select(\"main_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"main_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "strIndexer = StringIndexer(inputCol='item_id',outputCol='indexer_item_id')\n",
    "\n",
    "# lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "rf = RandomForestClassifier(labelCol='indexer_item_id',featuresCol='features', numTrees=1) #'item_id'\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexer_item_id\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[strIndexer, tokenizer, hashingTF,dt]) #, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf_filtered_1, sdf_filtered_2) = sdf_filtered.randomSplit([0.2,0.8])\n",
    "# (sdf_test_1, sdf_test_2) = sdf_test.randomSplit([0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(sdf_filtered_1)\n",
    "# prediction = model.transform(sdf_test)\n",
    "# prediction.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
